{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7483697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1106 21:24:11.970000 63781 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbf3025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4116e327edf04337a3c1118419fe43fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"lmarena-ai/arena-human-preference-140k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65a950b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'model_a', 'model_b', 'winner', 'evaluation_session_id', 'evaluation_order', 'conversation_a', 'conversation_b', 'full_conversation', 'conv_metadata', 'category_tag', 'language', 'is_code', 'timestamp'],\n",
      "    num_rows: 135634\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = ds['train']\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faab2e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              model_name  \\\n",
      "0                         gemini-2.5-pro   \n",
      "1              claude-3-5-haiku-20241022   \n",
      "2                                o3-mini   \n",
      "3  claude-sonnet-4-20250514-thinking-32k   \n",
      "4             claude-3-5-sonnet-20241022   \n",
      "\n",
      "                                        conversation  \\\n",
      "0  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "1  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "2  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "3  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "4  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "\n",
      "                               other_model    winner  \\\n",
      "0  claude-3-7-sonnet-20250219-thinking-32k   model_a   \n",
      "1               claude-3-5-sonnet-20241022       tie   \n",
      "2                          gemma-3n-e4b-it  both_bad   \n",
      "3                           gemini-2.5-pro   model_a   \n",
      "4                      mistral-medium-2505   model_b   \n",
      "\n",
      "                  evaluation_session_id  evaluation_order  \\\n",
      "0  a333a685-37f9-474d-b703-f079d8329552                 1   \n",
      "1  73e84b7b-d158-4d2d-9a7f-56286ec14564                 1   \n",
      "2  8132faad-e9c2-48ef-b608-718504d9ab75                 1   \n",
      "3  d1c66159-2b6f-4c09-a090-165520330cc3                 3   \n",
      "4  8c2ce87c-650c-4777-8503-65b5f797ec50                 1   \n",
      "\n",
      "                                   full_conversation  \\\n",
      "0  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "1  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "2  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "3  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "4  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "\n",
      "                                       conv_metadata  \\\n",
      "0  {'sum_assistant_a_tokens': 1854, 'header_count...   \n",
      "1  {'sum_assistant_a_tokens': 605, 'header_count_...   \n",
      "2  {'sum_assistant_a_tokens': 2216, 'header_count...   \n",
      "3  {'sum_assistant_a_tokens': 297, 'header_count_...   \n",
      "4  {'sum_assistant_a_tokens': 459, 'header_count_...   \n",
      "\n",
      "                                        category_tag language  is_code  \\\n",
      "0  {'creative_writing_v0.1': {'creative_writing':...       en    False   \n",
      "1  {'creative_writing_v0.1': {'creative_writing':...       pl     True   \n",
      "2  {'creative_writing_v0.1': {'creative_writing':...       en     True   \n",
      "3  {'creative_writing_v0.1': {'creative_writing':...       en    False   \n",
      "4  {'creative_writing_v0.1': {'creative_writing':...       de    False   \n",
      "\n",
      "                   timestamp                                    id  \n",
      "0 2025-05-29 14:48:41.602925  c4b9710c-8d64-4bee-a0b0-94637ae4cc65  \n",
      "1 2025-07-07 02:04:51.977824  7c44a466-fd07-4992-9764-176b75746a07  \n",
      "2 2025-07-12 21:43:23.646116  46e04015-dc69-442a-8b37-3caea0336541  \n",
      "3 2025-07-09 19:05:16.179693  4a4380bb-bbdb-495f-8e09-39a08d88a28f  \n",
      "4 2025-05-22 10:34:25.921196  ce23b477-d72c-4048-a01d-c3d26c80ad71  \n"
     ]
    }
   ],
   "source": [
    "def reorganize(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    # Create model_a rows\n",
    "    df_a = df.rename(columns={\n",
    "        'model_a': 'model_name',\n",
    "        'conversation_a': 'conversation',\n",
    "        'model_b': 'other_model'\n",
    "    })[['model_name', 'conversation', 'other_model', 'winner', 'evaluation_session_id', \n",
    "        'evaluation_order', 'full_conversation', 'conv_metadata', 'category_tag', \n",
    "        'language', 'is_code', 'timestamp', 'id']]\n",
    "    # Create model_b rows\n",
    "    df_b = df.rename(columns={\n",
    "        'model_b': 'model_name',\n",
    "        'conversation_b': 'conversation',\n",
    "        'model_a': 'other_model'\n",
    "    })[['model_name', 'conversation', 'other_model', 'winner', 'evaluation_session_id', \n",
    "        'evaluation_order', 'full_conversation', 'conv_metadata', 'category_tag', \n",
    "        'language', 'is_code', 'timestamp', 'id']]\n",
    "    # Combine and convert back\n",
    "    df_combined = pd.concat([df_a, df_b], ignore_index=True)\n",
    "    return df_combined\n",
    "\n",
    "reorganized = reorganize(train_ds)\n",
    "print(reorganized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e319981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "num_unique_models = len(set(reorganized['model_name']))\n",
    "print(num_unique_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbbdf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model to label mapping: {'amazon-nova-experimental-chat-05-14': 0, 'amazon.nova-pro-v1:0': 1, 'chatgpt-4o-latest-20250326': 2, 'claude-3-5-haiku-20241022': 3, 'claude-3-5-sonnet-20241022': 4, 'claude-3-7-sonnet-20250219': 5, 'claude-3-7-sonnet-20250219-thinking-32k': 6, 'claude-opus-4-20250514': 7, 'claude-opus-4-20250514-thinking-16k': 8, 'claude-sonnet-4-20250514': 9, 'claude-sonnet-4-20250514-thinking-32k': 10, 'command-a-03-2025': 11, 'deepseek-r1-0528': 12, 'deepseek-v3-0324': 13, 'gemini-2.0-flash-001': 14, 'gemini-2.0-flash-thinking-exp-01-21': 15, 'gemini-2.5-flash': 16, 'gemini-2.5-flash-lite-preview-06-17-thinking': 17, 'gemini-2.5-flash-preview-04-17': 18, 'gemini-2.5-pro': 19, 'gemini-2.5-pro-preview-03-25': 20, 'gemini-2.5-pro-preview-05-06': 21, 'gemma-3-27b-it': 22, 'gemma-3n-e4b-it': 23, 'gpt-4.1-2025-04-14': 24, 'gpt-4.1-mini-2025-04-14': 25, 'gpt-4o-2024-11-20': 26, 'gpt-4o-mini-2024-07-18': 27, 'grok-3-mini-beta': 28, 'grok-3-mini-high': 29, 'grok-3-preview-02-24': 30, 'grok-4-0709': 31, 'hunyuan-turbos-20250416': 32, 'kimi-k2-0711-preview': 33, 'llama-3.3-70b-instruct': 34, 'llama-4-maverick-03-26-experimental': 35, 'llama-4-maverick-17b-128e-instruct': 36, 'llama-4-scout-17b-16e-instruct': 37, 'magistral-medium-2506': 38, 'minimax-m1': 39, 'mistral-medium-2505': 40, 'mistral-small-2506': 41, 'mistral-small-3.1-24b-instruct-2503': 42, 'o3-2025-04-16': 43, 'o3-mini': 44, 'o4-mini-2025-04-16': 45, 'qwen-max-2025-01-25': 46, 'qwen3-235b-a22b': 47, 'qwen3-235b-a22b-instruct-2507': 48, 'qwen3-235b-a22b-no-thinking': 49, 'qwen3-30b-a3b': 50, 'qwen3-coder-480b-a35b-instruct': 51, 'qwq-32b': 52}\n",
      "                              model_name  \\\n",
      "0                         gemini-2.5-pro   \n",
      "1              claude-3-5-haiku-20241022   \n",
      "2                                o3-mini   \n",
      "3  claude-sonnet-4-20250514-thinking-32k   \n",
      "4             claude-3-5-sonnet-20241022   \n",
      "\n",
      "                                        conversation  \\\n",
      "0  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "1  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "2  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "3  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "4  [{'role': 'user', 'content': [{'type': 'text',...   \n",
      "\n",
      "                               other_model    winner  \\\n",
      "0  claude-3-7-sonnet-20250219-thinking-32k   model_a   \n",
      "1               claude-3-5-sonnet-20241022       tie   \n",
      "2                          gemma-3n-e4b-it  both_bad   \n",
      "3                           gemini-2.5-pro   model_a   \n",
      "4                      mistral-medium-2505   model_b   \n",
      "\n",
      "                  evaluation_session_id  evaluation_order  \\\n",
      "0  a333a685-37f9-474d-b703-f079d8329552                 1   \n",
      "1  73e84b7b-d158-4d2d-9a7f-56286ec14564                 1   \n",
      "2  8132faad-e9c2-48ef-b608-718504d9ab75                 1   \n",
      "3  d1c66159-2b6f-4c09-a090-165520330cc3                 3   \n",
      "4  8c2ce87c-650c-4777-8503-65b5f797ec50                 1   \n",
      "\n",
      "                                   full_conversation  \\\n",
      "0  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "1  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "2  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "3  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "4  [{'user': {'role': 'user', 'content': [{'type'...   \n",
      "\n",
      "                                       conv_metadata  \\\n",
      "0  {'sum_assistant_a_tokens': 1854, 'header_count...   \n",
      "1  {'sum_assistant_a_tokens': 605, 'header_count_...   \n",
      "2  {'sum_assistant_a_tokens': 2216, 'header_count...   \n",
      "3  {'sum_assistant_a_tokens': 297, 'header_count_...   \n",
      "4  {'sum_assistant_a_tokens': 459, 'header_count_...   \n",
      "\n",
      "                                        category_tag language  is_code  \\\n",
      "0  {'creative_writing_v0.1': {'creative_writing':...       en    False   \n",
      "1  {'creative_writing_v0.1': {'creative_writing':...       pl     True   \n",
      "2  {'creative_writing_v0.1': {'creative_writing':...       en     True   \n",
      "3  {'creative_writing_v0.1': {'creative_writing':...       en    False   \n",
      "4  {'creative_writing_v0.1': {'creative_writing':...       de    False   \n",
      "\n",
      "                   timestamp                                    id  label  \n",
      "0 2025-05-29 14:48:41.602925  c4b9710c-8d64-4bee-a0b0-94637ae4cc65     19  \n",
      "1 2025-07-07 02:04:51.977824  7c44a466-fd07-4992-9764-176b75746a07      3  \n",
      "2 2025-07-12 21:43:23.646116  46e04015-dc69-442a-8b37-3caea0336541     44  \n",
      "3 2025-07-09 19:05:16.179693  4a4380bb-bbdb-495f-8e09-39a08d88a28f     10  \n",
      "4 2025-05-22 10:34:25.921196  ce23b477-d72c-4048-a01d-c3d26c80ad71      4  \n"
     ]
    }
   ],
   "source": [
    "unique_models = sorted(set(reorganized['model_name']))\n",
    "model_to_label = {model: idx for idx, model in enumerate(unique_models)}\n",
    "label_to_model = {idx: model for model, idx in model_to_label.items()}\n",
    "reorganized['label'] = reorganized['model_name'].map(model_to_label)\n",
    "\n",
    "print(f\"Model to label mapping: {model_to_label}\")\n",
    "print(reorganized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75bada08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_conversation(conversation_list):\n",
    "    \"\"\"\n",
    "    Convert conversation list format to a single text string\n",
    "    \"\"\"\n",
    "    \n",
    "    text_parts = []\n",
    "    for turn in conversation_list:\n",
    "        if isinstance(turn, dict) and 'content' in turn and 'role' in turn:\n",
    "            role = turn['role']\n",
    "            # Extract text from content list\n",
    "            for content_item in turn['content']:\n",
    "                if isinstance(content_item, dict) and content_item.get('text'):\n",
    "                    text = content_item['text']\n",
    "                    text_parts.append(f\"{role}: {text}.\")\n",
    "    return \" \".join(text_parts) if text_parts else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc92a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: Compare Tormenta20 with DnD5e. assistant: Excellent question! Comparing **Tormenta20 (T20)** and **Dungeons & Dragons 5th Edition (D&D 5e)** is a fantastic way to understand different approaches to the d20 system. While they share a common ancestor (D&D 3rd Edition), they have evolved in vastly different directions.\n",
      "\n",
      "Here is a detailed comparison, broken down by key aspects of the games.\n",
      "\n",
      "---\n",
      "\n",
      "### **High-Level Philosophy**\n",
      "\n",
      "*   **D&D 5e:** Aims for **accessibility, streamlined play, and heroic fantasy**. It simplifies many of the complex rules of older editions to be welcoming to new players. Its design philosophy is \"rulings, not rules,\" encouraging Dungeon Masters to make calls on the fly. It is built to be setting-agnostic, though the Forgotten Realms is its default.\n",
      "*   **Tormenta20:** Aims for **deep character customization, high-power fantasy, and an \"anime/JRPG\" feel**. It offers players a vast menu of options to build a unique character from level 1. The power level escalates quickly, and characters often feel like epic heroes from a shonen anime or a Final Fantasy game. It is deeply tied to its unique setting, Arton.\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Mechanic: The d20 Roll**\n",
      "\n",
      "*   **D&D 5e:** The core is **d20 + Ability Modifier + Proficiency Bonus** vs. a Difficulty Class (DC) or Armor Class (AC).\n",
      "    *   **Bounded Accuracy:** A key design principle. Bonuses don't get astronomically high, which keeps lower-level monsters a potential threat for longer and makes the math simpler.\n",
      "    *   **Advantage/Disadvantage:** The primary way to handle situational modifiers. Instead of adding a +2 or -2, you roll two d20s and take the higher (Advantage) or lower (Disadvantage) result.\n",
      "*   **Tormenta20:** The core is **d20 + Ability Modifier + Other Bonuses** vs. a Difficulty (DT in Portuguese).\n",
      "    *   **No Proficiency Bonus:** This is a massive difference. Your core bonus comes from your ability scores and training in a skill (which gives a flat bonus). Most of your power comes from the **Powers System**.\n",
      "    *   **Situational Modifiers:** T20 uses traditional numerical bonuses and penalties (+2, -5, etc.) much more frequently than 5e.\n",
      "\n",
      "---\n",
      "\n",
      "### **Character Creation & Progression**\n",
      "\n",
      "This is the single biggest point of divergence.\n",
      "\n",
      "*   **D&D 5e:**\n",
      "    *   **Structure:** Race + Class + Background.\n",
      "    *   **Customization:** Your main choices are your class, your subclass (usually chosen at levels 1, 2, or 3), and your spell selection.\n",
      "    *   **Progression:** You gain class features at set levels. At specific intervals (levels 4, 8, 12, etc.), you get an Ability Score Improvement (ASI) or can choose an optional **Feat**. Many players feel some levels are \"dead levels\" where you only gain hit points.\n",
      "*   **Tormenta20:**\n",
      "    *   **Structure:** Race + Class.\n",
      "    *   **The Powers System (Poderes):** This is the heart of Tormenta20. **At every single level, you choose a Power.** These can be general powers, powers from your class, or powers from your race. This creates an \"à la carte\" or \"build-your-own\" character system.\n",
      "    *   **Progression:** You are constantly making meaningful choices. There are no \"dead levels.\" Two Fighters of the same level can play completely differently based on their Power choices. This allows for incredible build diversity and theory-crafting, similar to games like *Pathfinder*.\n",
      "\n",
      "---\n",
      "\n",
      "### **Magic System**\n",
      "\n",
      "Another huge difference that defines the feel of each game.\n",
      "\n",
      "*   **D&D 5e:**\n",
      "    *   **Vancian Magic (Spell Slots):** Casters have a specific number of spell slots per spell level, which they regain after a long rest. You can cast a 1st-level spell using a 1st-level slot, a 2nd-level spell using a 2nd-level slot, and so on. It's a resource management system based on preparing for the adventuring day.\n",
      "*   **Tormenta20:**\n",
      "    *   **Mana Points (MP):** This system is straight out of JRPGs. Every caster has a pool of Mana Points (called *Mana*). Every spell has an MP cost. You can cast any spell you know as long as you have the MP to pay for it.\n",
      "    *   **Enhancements:** Most spells have optional enhancements. You can spend more MP when casting a spell to make it more powerful (e.g., increase its area, damage, or duration). This gives casters incredible flexibility on the fly.\n",
      "\n",
      "---\n",
      "\n",
      "### **Combat and Power Level**\n",
      "\n",
      "*   **D&D 5e:**\n",
      "    *   **Pacing:** Combat is tactical and attrition-based. Managing resources (HP, spell slots, limited-use abilities) is key.\n",
      "    *   **Power Curve:** The power curve is relatively gradual. A 1st-level character is fragile, while a 20th-level character is a world-shaking force, but the journey is long.\n",
      "    *   **Defense:** Primarily based on **Armor Class (AC)**.\n",
      "*   **Tormenta20:**\n",
      "    *   **Pacing:** Combat is faster, more explosive, and more \"spiky.\" Characters can unleash incredible amounts of damage or powerful effects from early levels. It feels more like an action movie.\n",
      "    *   **Power Curve:** The power curve is extremely steep. A 1st-level T20 character often feels as competent as a 3rd-level 5e character. The numbers (HP, damage, bonuses) get much bigger, much faster.\n",
      "    *   **Defense:** Based on a single stat, **Defense (Defesa)**, but it's calculated differently, incorporating armor, shields, and your Dexterity modifier. Saving throws are also different: **Fortitude, Reflexes, and Will (Vontade)**, which are more prominent than in 5e.\n",
      "\n",
      "---\n",
      "\n",
      "### **Setting Integration**\n",
      "\n",
      "*   **D&D 5e:** Very loose integration. The rules are designed to be used in any fantasy setting with minimal changes. The default Forgotten Realms is rich, but you can ignore it completely without affecting the mechanics.\n",
      "*   **Tormenta20:** Extremely tight integration. The world of **Arton** is baked into the rules. The races (like the half-demon *Lefou*), the classes, the gods, and especially the cosmic threat of the **Tormenta** (a reality-devouring storm of red lightning and demons) are all reflected in the Powers and mechanics. Playing T20 without its setting is possible, but you lose a massive amount of its unique flavor.\n",
      "\n",
      "---\n",
      "\n",
      "### **Summary Table**\n",
      "\n",
      "| Feature | D&D 5th Edition | Tormenta20 |\n",
      "| :--- | :--- | :--- |\n",
      "| **Philosophy** | Accessible, Heroic Fantasy | High-Power, \"Anime/JRPG\" Feel |\n",
      "| **Core Mechanic** | d20 + Ability + Proficiency | d20 + Ability + Other Bonuses |\n",
      "| **Customization** | Class, Subclass, Feats (optional) | **Powers System (a new power every level)** |\n",
      "| **Magic System** | Spell Slots (Vancian) | **Mana Points (MP)** |\n",
      "| **Power Level** | Gradual, \"bounded\" power curve | Steep, explosive power curve |\n",
      "| **Feel of Combat** | Tactical, resource management | Fast, high-damage, over-the-top |\n",
      "| **Setting** | Setting-agnostic (default is Forgotten Realms) | Deeply tied to the world of Arton |\n",
      "| **Complexity** | Low to Medium (streamlined rules) | Medium to High (in character creation) |\n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion: Which One is Right for You?**\n",
      "\n",
      "**Play D&D 5e if:**\n",
      "*   You are new to tabletop RPGs.\n",
      "*   You want a game with a massive amount of official content, streaming shows, and online resources.\n",
      "*   You prefer a more grounded (but still heroic) fantasy experience.\n",
      "*   You value streamlined rules and quick character creation over deep, granular customization.\n",
      "\n",
      "**Play Tormenta20 if:**\n",
      "*   You love theory-crafting and creating highly unique character builds.\n",
      "*   You want to feel incredibly powerful from the early levels.\n",
      "*   You enjoy the aesthetics and power fantasy of JRPGs and anime.\n",
      "*   You are a D&D 5e veteran looking for a system with more mechanical depth, player choice, and a different flavor of fantasy..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(clean_conversation(reorganized['conversation'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0df731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         user: Compare Tormenta20 with DnD5e. assistant...\n",
      "1         user: jak w prompcie określić precyzyjnie para...\n",
      "2         user: Solve the following game of Freecell. Ca...\n",
      "3         user: What about 1 year in Sweden with proof e...\n",
      "4         user: Gibt es bei Kleidungs Reinigung verschie...\n",
      "                                ...                        \n",
      "271263    user: co się stało na placu tiananmen?. assist...\n",
      "271264    user: rtss не выводит 1% фпс. assistant: Пробл...\n",
      "271265    user: czy odpowiadasz na pytania zadane po pol...\n",
      "271266    user: Проверь этот код что он делает?\\n\\n     ...\n",
      "271267    user: What does it mean to say God's transcend...\n",
      "Name: conversation_text, Length: 271268, dtype: object\n"
     ]
    }
   ],
   "source": [
    "reorganized['conversation_text'] = reorganized['conversation'].apply(clean_conversation)\n",
    "print(reorganized['conversation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0974231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after cleaning: 271268\n",
      "1905943\n"
     ]
    }
   ],
   "source": [
    "reorganized = reorganized[reorganized['conversation_text'].notna()]\n",
    "reorganized = reorganized[reorganized['conversation_text'].str.len() > 10]\n",
    "\n",
    "print(f\"Rows after cleaning: {len(reorganized)}\")\n",
    "print(max(reorganized['conversation_text'].str.len()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "383e18c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1982 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1982\n",
      "1     777\n",
      "2    2167\n",
      "3     368\n",
      "4     605\n",
      "Name: conversation_text, dtype: int64\n",
      "\n",
      "Token count statistics:\n",
      "count    271268.000000\n",
      "mean       1532.890835\n",
      "std        2533.892882\n",
      "min           4.000000\n",
      "25%         408.000000\n",
      "50%         935.000000\n",
      "75%        1790.000000\n",
      "max      235390.000000\n",
      "Name: conversation_text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens using BERT tokenizer\"\"\"\n",
    "    if text is None:\n",
    "        return 0\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "num_tokens = reorganized['conversation_text'].apply(count_tokens)\n",
    "\n",
    "print(num_tokens.head())\n",
    "print(f\"\\nToken count statistics:\")\n",
    "print(num_tokens.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "288f9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81334\n",
      "368\n"
     ]
    }
   ],
   "source": [
    "data_short = reorganized[num_tokens < 500]\n",
    "print(len(data_short))\n",
    "print(count_tokens(data_short.iloc[0]['conversation_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19e354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(data_short[['conversation_text', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21da296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 58560\n",
      "Val set size: 6507\n",
      "Test set size: 16267\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation/test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Further split train into train/validation\n",
    "train_val_split = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_val_split['train']\n",
    "val_dataset = train_val_split['test']\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Val set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df6ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conversation_text', 'label', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7485f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74403e9925c495c8d9c761c76a07f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/81334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('./LMarena_short')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfb295b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['conversation_text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512  # Adjust based on your conversation lengths\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f99eb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52400e34b50f48688c67e0df813ad457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58560 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d69c84bbd514d6c83b5022c4b2bb5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e137ae58d8d946aba41433e9ef9655f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize all datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text column (we only need input_ids, attention_mask, and labels)\n",
    "train_dataset = train_dataset.remove_columns(['conversation_text'])\n",
    "val_dataset = val_dataset.remove_columns(['conversation_text'])\n",
    "test_dataset = test_dataset.remove_columns(['conversation_text'])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch')\n",
    "val_dataset.set_format('torch')\n",
    "test_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f0266c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f5fa8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=53\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b8d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy='epoch',  # Evaluate at the end of each epoch\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    learning_rate=2e-5,\n",
    "    seed=42,\n",
    "    push_to_hub=False,  # Set to True if you want to push to Hugging Face Hub\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35b500e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30fb6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhanghantao/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='10980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    7/10980 04:00 < 146:38:07, 0.02 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2250\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:2607\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2603\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[1;32m   2605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m   2609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped:\n\u001b[1;32m   2612\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:133\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    132\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     adam(\n\u001b[1;32m    248\u001b[0m         params_with_grad,\n\u001b[1;32m    249\u001b[0m         grads,\n\u001b[1;32m    250\u001b[0m         exp_avgs,\n\u001b[1;32m    251\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    252\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    253\u001b[0m         state_steps,\n\u001b[1;32m    254\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    255\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    256\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    257\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    258\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    259\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    260\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    261\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    262\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    263\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    264\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    265\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    266\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    267\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    268\u001b[0m         decoupled_weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoupled_weight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 953\u001b[0m func(\n\u001b[1;32m    954\u001b[0m     params,\n\u001b[1;32m    955\u001b[0m     grads,\n\u001b[1;32m    956\u001b[0m     exp_avgs,\n\u001b[1;32m    957\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    958\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    959\u001b[0m     state_steps,\n\u001b[1;32m    960\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    961\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    962\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    963\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    964\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    965\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    966\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    967\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    968\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    969\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    970\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    971\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[1;32m    972\u001b[0m     decoupled_weight_decay\u001b[38;5;241m=\u001b[39mdecoupled_weight_decay,\n\u001b[1;32m    973\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:537\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 537\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
